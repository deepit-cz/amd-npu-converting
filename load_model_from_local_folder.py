"""
Script to load Qwen3-Coder-30B model from a LOCAL folder
Use this after you've downloaded the model to a local directory
"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import os

# Path to your local model folder
# Change this to match where you saved the model
MODEL_PATH = "./models/Qwen3-Coder-30B-A3B-Instruct-int4-AutoRound"

# Check if model exists
if not os.path.exists(MODEL_PATH):
    print(f"Error: Model not found at {os.path.abspath(MODEL_PATH)}")
    print("\nPlease either:")
    print("1. Download the model first using download_and_use_qwen3_coder_local.py")
    print("2. Update MODEL_PATH in this script to point to your model location")
    print("\nOr use download_and_use_qwen3_coder.py for automatic download")
    exit(1)

print(f"Loading model from: {os.path.abspath(MODEL_PATH)}\n")

# Load from local folder
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)

# Check for available accelerators (CUDA, Intel XPU, or CPU)
has_cuda = torch.cuda.is_available()
has_intel_gpu = False
intel_gpu_device = None

# Check for Intel GPU (XPU) - requires intel-extension-for-pytorch
try:
    import intel_extension_for_pytorch as ipex
    if hasattr(ipex, 'xpu') and ipex.xpu.is_available():
        has_intel_gpu = True
        intel_gpu_device = torch.device("xpu:0")
        print(f"âœ“ Intel GPU (XPU) detected!")
        print(f"  Using Intel GPU for faster conversion (4-12 hours vs 12-48+ hours on CPU)")
except ImportError:
    print("â„¹ Intel Extension for PyTorch not installed.")
    print("  To use Intel GPU: Install from https://github.com/intel/intel-extension-for-pytorch")
    print("  Or use OpenVINO for Intel GPU acceleration")
except Exception as e:
    print(f"Note: Intel GPU detection failed: {e}")

if has_cuda:
    print(f"NVIDIA GPU detected: {torch.cuda.get_device_name(0)}")
    print("Loading model on NVIDIA GPU...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    # Convert to bfloat16 after loading if needed
    if hasattr(model, 'to'):
        model = model.to(torch.bfloat16)
elif has_intel_gpu:
    print("Loading model on Intel GPU (XPU)...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        device_map={"": "xpu:0"},  # Use Intel GPU
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    # Convert to bfloat16 after loading if needed
    if hasattr(model, 'to'):
        try:
            model = model.to(intel_gpu_device)
            model = model.to(torch.bfloat16)
        except Exception as e:
            print(f"Note: Could not convert to bfloat16: {e}")
            print("Using model's default dtype")
else:
    print("No GPU detected, loading on CPU...")
    print("âš  For faster conversion, consider using Intel GPU (install intel-extension-for-pytorch)")
    # For CPU, use explicit CPU device_map to avoid disk offloading
    # MoE models may not support dtype parameter, so we load first then convert
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        device_map={"": "cpu"},  # Explicit CPU mapping
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    # Convert to bfloat16 after loading if needed
    if hasattr(model, 'to'):
        try:
            model = model.to(torch.bfloat16)
        except Exception as e:
            print(f"Note: Could not convert to bfloat16: {e}")
            print("Using model's default dtype")

print("Model loaded successfully!")
print(f"Model device: {next(model.parameters()).device}")
print(f"Model dtype: {next(model.parameters()).dtype}\n")

# ============================================================================
# Convert to ONNX Format (Required before NPU compilation)
# ============================================================================
# To use the model on NPU, you need to convert it to ONNX format first.
# This conversion may require significant memory and time for a 30B model.
# 
# Uncomment the following code to convert the model to ONNX:
#
import onnx
from optimum.onnxruntime import ORTModelForCausalLM

# Output path for ONNX model
onnx_model_path = "./models/qwen3-coder-30b-onnx"
os.makedirs(onnx_model_path, exist_ok=True)

print("=" * 70)
print("Starting ONNX Conversion using Optimum")
print("=" * 70)
print(f"Output path: {os.path.abspath(onnx_model_path)}")

# Check if model is quantized
is_quantized = "int4" in MODEL_PATH.lower() or "quant" in MODEL_PATH.lower()
if is_quantized:
    print("\nâš  IMPORTANT: You're exporting a QUANTIZED model (int4-AutoRound)")
    print("   - Quantized models may cause precision errors when exported to ONNX")
    print("   - The quantization format may not be preserved in ONNX")
    print("   - RECOMMENDED: Export non-quantized model, then quantize the ONNX model")
    print("   - OR: The ONNX model will need to be re-quantized after export")
    print("   - Precision errors (float32/float16 mismatch) are common with quantized exports")

print("\nâš  TIME ESTIMATE for 30B Model:")
print("   - CPU-only: 12-48+ hours (possibly days)")
print("   - Intel GPU (Arc): 4-12 hours (much faster!)")
print("   - NVIDIA GPU: 2-8 hours")
print("   - MoE models are more complex and take longer")
print("\nðŸ’¡ TIPS:")
print("   - Intel GPU acceleration: Install 'intel-extension-for-pytorch'")
print("   - Let it run overnight or over a weekend")
print("   - Monitor memory usage (32GB may be tight)")
print("   - The process will create checkpoint files you can resume")
if is_quantized:
    print("   - If you get precision errors, re-quantize the ONNX model using AMD Quark")
print("=" * 70 + "\n")

# Use Optimum's ONNX exporter which handles transformers models better
# This avoids the dynamic shape issues with torch.onnx.export
try:
    import time
    start_time = time.time()
    
    print("Exporting model to ONNX format using Optimum...")
    print("Note: For MoE models, this process may take significant time and memory...")
    print(f"Started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # Optimum's export handles MoE models and dynamic shapes better
    # The export=True parameter triggers the conversion
    # Add export_kwargs to handle precision issues
    export_kwargs = {
        "opset": 17,  # ONNX opset version 17 for Ryzen AI compatibility
        # Try to use consistent precision (float16) to avoid type mismatches
    }
    
    ort_model = ORTModelForCausalLM.from_pretrained(
        MODEL_PATH,
        export=True,  # This triggers the export
        export_kwargs=export_kwargs,
        trust_remote_code=True
    )
    
    elapsed_time = time.time() - start_time
    hours = int(elapsed_time // 3600)
    minutes = int((elapsed_time % 3600) // 60)
    seconds = int(elapsed_time % 60)
    print(f"\nâœ“ Export completed in {hours}h {minutes}m {seconds}s")
    
    # Save the ONNX model
    ort_model.save_pretrained(onnx_model_path)
    print(f"\nâœ“ Model successfully exported to ONNX: {onnx_model_path}")
    
    # Verify ONNX model
    try:
        # Check if main model file exists
        model_files = [f for f in os.listdir(onnx_model_path) if f.endswith('.onnx')]
        if model_files:
            total_size = sum(os.path.getsize(os.path.join(onnx_model_path, f)) for f in model_files) / (1024**3)
            print(f"âœ“ ONNX model files created: {len(model_files)} file(s)")
            print(f"  Total size: {total_size:.2f} GB")
            
            # Try to validate the ONNX model
            try:
                print("\nValidating ONNX model...")
                onnx_file = os.path.join(onnx_model_path, model_files[0])
                onnx_model = onnx.load(onnx_file)
                onnx.checker.check_model(onnx_model)
                print("âœ“ ONNX model validation passed")
                
                # Check for precision issues
                print("\nChecking for precision issues...")
                try:
                    from onnxruntime import InferenceSession
                    # Try to create a session to catch runtime precision errors
                    test_session = InferenceSession(onnx_file, providers=['CPUExecutionProvider'])
                    print("âœ“ ONNX Runtime can load the model successfully")
                    del test_session
                except Exception as runtime_error:
                    if "Type Error" in str(runtime_error) or "precision" in str(runtime_error).lower():
                        print(f"âš  Precision mismatch detected: {runtime_error}")
                        print("\nðŸ’¡ To fix precision issues, you can:")
                        print("   1. Use ONNX Simplifier: pip install onnx-simplifier")
                        print("   2. Use quantization tools to convert to consistent precision")
                        print("   3. The model may still work with specific ONNX Runtime providers")
                    else:
                        raise
            except Exception as ve:
                print(f"âš  Warning: ONNX model validation failed: {ve}")
                print("  The model may still work, but you may need to fix precision manually")
        else:
            print("âš  Warning: No .onnx files found in output directory")
            print("  Checking for other files...")
            all_files = os.listdir(onnx_model_path)
            if all_files:
                print(f"  Found files: {all_files[:10]}")  # Show first 10 files
    except Exception as e:
        print(f"âš  Warning: Could not verify ONNX model: {e}")
    
except Exception as e:
    error_msg = str(e)
    if "Type Error" in error_msg or "precision" in error_msg.lower() or "float" in error_msg.lower():
        print(f"\nâš  Precision/Type Error during ONNX export: {e}")
        if is_quantized:
            print("\nâš  ROOT CAUSE: Exporting a quantized (int4) model to ONNX")
            print("   - The AutoRound int4 quantization format is not preserved in ONNX")
            print("   - ONNX export converts quantized weights to float types inconsistently")
            print("   - This causes float32/float16 mismatches")
        else:
            print("\nThis is a common issue with mixed precision models (float32/float16).")
        print("\nðŸ”§ SOLUTIONS:")
        if is_quantized:
            print("1. RE-QUANTIZE the ONNX model using AMD Quark (recommended):")
            print("   - This will convert the ONNX model to consistent INT4/BF16 precision")
            print("   - This fixes the precision mismatch AND prepares for NPU")
            print("2. Export the non-quantized model first, then quantize the ONNX model")
            print("3. Use ONNX Runtime with providers that handle mixed precision:")
            print("   - Try: providers=['VitisAIExecutionProvider'] (may handle it better)")
        else:
            print("1. Use ONNX Runtime with specific providers that handle mixed precision:")
            print("   - Try: providers=['CPUExecutionProvider', 'CUDAExecutionProvider']")
            print("2. Fix the ONNX model precision manually using ONNX tools")
            print("3. Re-export with consistent precision settings")
            print("4. Use quantization to convert to a single precision (INT4/INT8)")
        print("\nThe exported model files are still created and may work with different providers.")
    else:
        print(f"\nâœ— Error during ONNX export with Optimum: {e}")
        print("\nNote: MoE (Mixture of Experts) models can be challenging to export to ONNX.")
        print("The dynamic routing in MoE models creates data-dependent control flow.")
        print("\nAlternative options:")
        print("1. Use the model in PyTorch format directly (no ONNX conversion needed)")
        print("2. Try exporting individual components separately")
        print("3. Use a non-MoE variant of the model if available")
        print("4. Consider using AMD's Quark tools which may handle MoE models better")
    
    # Don't raise - let the user know the files were created even if there are warnings
    if "Type Error" in error_msg or "precision" in error_msg.lower():
        print("\nâš  Continuing despite precision warning - model files are saved.")
    else:
        raise
# 
# # Quantize the Model (Recommended for NPU)
# # After converting to ONNX, you should quantize the model:
# # - Use AMD Quark Model Optimization Library to quantize to INT4 or BF16
# # - BF16 is supported for transformer models on Ryzen AI NPU
# # - INT4 quantization can reduce memory requirements significantly
# # - Quantized model path: "./models/qwen3-coder-30b-quantized.onnx"
# ============================================================================

# ============================================================================
# Compile for NPU (Alternative approach using ONNX Runtime with Vitis AI EP)
# ============================================================================
# Note: This requires the model to be converted to ONNX format first.
# The model must be quantized and exported as ONNX before using this approach.
# 
# IMPORTANT: If you encountered precision errors (float32/float16 mismatch),
# quantization will fix this by converting to a consistent precision (INT4/BF16).
# 
# To use NPU execution:
# 1. Convert the model to ONNX format (see section above)
# 2. Quantize the model (INT4 or BF16 recommended for NPU) - THIS FIXES PRECISION ISSUES
# 3. Use the code below to create an inference session with Vitis AI EP
#
# Uncomment the following code if you have an ONNX model ready:
#
# from onnxruntime import InferenceSession
# from onnxruntime.vitisai import VitisAIExecutionProvider
# 
# # Create inference session with Vitis AI EP
# session = InferenceSession(
#     "qwen3-coder-30b-quantized.onnx",
#     providers=['VitisAIExecutionProvider']
# )
#
# # For hybrid execution (NPU + GPU/CPU):
# # session = InferenceSession(
# #     "qwen3-coder-30b.onnx",
# #     providers=['VitisAIExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
# # )
#
# # Run inference on NPU:
# # prompt = "Write a Python function to calculate fibonacci numbers"
# # inputs = tokenizer(prompt, return_tensors="np")
# # outputs = session.run(None, dict(inputs))
# ============================================================================

# Example usage
prompt = "Write a Python function to check if a number is prime."
print(f"Prompt: {prompt}\n")

messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    temperature=0.7,
    do_sample=True
)

output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
content = tokenizer.decode(output_ids, skip_special_tokens=True)

print("Generated response:")
print(content)
